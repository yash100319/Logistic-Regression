{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAh0CTD5sC7v"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assignment Questions\n",
        "#Theoretical\n",
        "1. What is Logistic Regression, and how does it differ from Linear Regression ?\n",
        "- Logistic Regression and Linear Regression are both supervised learning algorithms used for prediction tasks, but they are used for different types of problems and have distinct characteristics\n",
        " Logistic Regression\n",
        "Purpose:\n",
        "Used for classification problems ‚Äî especially binary classification (e.g., spam or not spam, yes or no, pass or fail).\n",
        "2. What is the mathematical equation of Logistic Regression ?\n",
        "- The mathematical equation of Logistic Regression is based on the logistic (sigmoid) function applied to a linear combination of input features\n",
        "1. Logistic Function:\n",
        "2. Linear Combination (Model Input):\n",
        "3. Why do we use the Sigmoid function in Logistic Regression ?\n",
        "- We use the sigmoid function in logistic regression because it maps any real-valued number into a range between 0 and 1, which is perfect for binary classification tasks.\n",
        "\n",
        "Logistic regression predicts probabilities for binary outcomes (e.g., 0 or 1, yes or no, spam or not spam).\n",
        "\n",
        "These probabilities must be between 0 and 1.\n",
        "4. What is the cost function of Logistic Regression ?\n",
        "- The cost function of logistic regression is based on the log loss, also known as binary cross-entropy. It's designed to measure how well the predicted probabilities match the actual binary outcomes (0 or 1).\n",
        "\n",
        "1. Logistic Regression Hypothesis\n",
        "2. Cost Function (Binary Cross-Entropy)\n",
        "5. What is Regularization in Logistic Regression? Why is it needed ?\n",
        "- Regularization in Logistic Regression is a technique used to prevent overfitting by discouraging overly complex models. It does this by adding a penalty term to the cost (or loss) function used to train the model.\n",
        "Overfitting: When a logistic regression model is too complex (e.g. too many features or large coefficient values), it can perform well on the training data but poorly on unseen (test) data.\n",
        "\n",
        "Model Complexity: Without regularization, the model might assign large weights to some features, making the model too sensitive to noise or outliers.\n",
        "\n",
        "Generalization: Regularization helps the model generalize better to new data by keeping the weights smaller and more balanced.\n",
        "6.  Explain the difference between Lasso, Ridge, and Elastic Net regression ?\n",
        "- Lasso, Ridge, and Elastic Net are regularized regression techniques used to prevent overfitting and improve model generalization by adding a penalty to the loss function.\n",
        "7. When should we use Elastic Net instead of Lasso or Ridge ?\n",
        "- You should use Elastic Net instead of Lasso or Ridge when your data exhibits characteristics that require a balance between their strengths\n",
        "You have many correlated features\n",
        "Lasso tends to pick only one feature from a group of highly correlated variables and ignore the others.\n",
        "\n",
        "Ridge shrinks coefficients of correlated features towards each other.\n",
        "\n",
        "Elastic Net combines both: it allows for group selection (like Ridge) while also enforcing sparsity (like Lasso).\n",
        "\n",
        "You need both variable selection and regularization\n",
        "Lasso can set some coefficients to zero (variable selection), but performs poorly when\n",
        "ùëù\n",
        ">\n",
        "ùëõ\n",
        "p>n and features are correlated.\n",
        "\n",
        "Ridge performs better with multicollinearity and high-dimensional data but doesn't eliminate features.\n",
        "\n",
        "8. What is the impact of the regularization parameter (Œª) in Logistic Regression?\n",
        "Regularization adds a penalty to the loss function of the logistic regression to discourage large coefficients, thereby helping the model generalize better.\n",
        "\n",
        "The cost function with regularization becomes:\n",
        "\n",
        "L2 Regularization (Ridge):\n",
        "9. C What are the key assumptions of Logistic Regression?\n",
        "- 1. Binary or categorical dependent variable\n",
        "The outcome variable should be binary (e.g., 0/1, yes/no) for binary logistic regression. Multinomial logistic regression handles multiple categories.\n",
        "\n",
        "2. Independence of observations\n",
        "Observations should be independent of each other. Logistic regression does not account for correlated data (e.g., repeated measures or clustered data) without modification.\n",
        "10. C What are some alternatives to Logistic Regression for classification tasks ?\n",
        "- Decision Trees\n",
        "\n",
        "Simple, interpretable models that split data into subsets.\n",
        "\n",
        "Good for non-linear relationships.\n",
        "\n",
        "Random Forests\n",
        "\n",
        "Ensemble of decision trees.\n",
        "\n",
        "More robust and less prone to overfitting\n",
        "2. Support Vector Machines (SVM)\n",
        "Effective in high-dimensional spaces.\n",
        "\n",
        "Good for both linear and non-linear classification (via kernel trick).\n",
        "\n",
        "Can be computationally expensive for large datasets.\n",
        "11. What are Classification Evaluation Metrics ?\n",
        "- Classification evaluation metrics are quantitative measures used to assess the performance of classification models. They help you understand how well your model is doing at correctly predicting the classes (labels) of your data.\n",
        "12. How does class imbalance affect Logistic Regression ?\n",
        "- Biased Decision Boundary\n",
        "Logistic Regression tries to find a decision boundary that separates classes by maximizing the likelihood.\n",
        "\n",
        "When one class heavily dominates (majority class), the model tends to favor predicting the majority class to minimize overall error.\n",
        "\n",
        "This can lead to a decision boundary skewed towards the minority class, making the model less sensitive to the minority class.\n",
        "Poor Minority Class Prediction (Low Recall)\n",
        "Since the model is biased towards the majority class, it often misclassifies minority class samples.\n",
        "\n",
        "This causes low recall (true positive rate) for the minority class, meaning many minority class instances are missed.\n",
        "\n",
        "In domains like fraud detection or disease diagnosis, missing minority class instances can be very costly.\n",
        "13. What is Hyperparameter Tuning in Logistic Regression ?\n",
        "- Unlike model parameters (like the weights/coefficients learned from data), hyperparameters are set before training and guide how the model learns. Common hyperparameters for logistic regression include:\n",
        "\n",
        "Regularization strength (C): Controls how much to penalize large coefficients. Smaller values imply stronger regularization.\n",
        "\n",
        "Type of regularization: L1 (Lasso), L2 (Ridge), or Elastic Net.\n",
        "\n",
        "Solver: The algorithm used to optimize the model (e.g., ‚Äòliblinear‚Äô, ‚Äòlbfgs‚Äô, ‚Äòsaga‚Äô).\n",
        "\n",
        "Maximum iterations: Number of iterations for the solver to converge.\n",
        "14. What are different solvers in Logistic Regression? Which one should be used\n",
        "- liblinear\n",
        "\n",
        "Uses a coordinate descent algorithm.\n",
        "\n",
        "Good for small datasets and binary classification problems.\n",
        "\n",
        "Can handle L1 and L2 regularization.\n",
        "\n",
        "Does not scale well to large datasets.\n",
        "\n",
        "newton-cg\n",
        "\n",
        "Based on Newton's method using a conjugate gradient solver.\n",
        "\n",
        "Supports only L2 regularization.\n",
        "\n",
        "Works well for large datasets.\n",
        "\n",
        "Supports multinomial (multiclass) classification.\n",
        "15. How is Logistic Regression extended for multiclass classification?\n",
        "- 1. One-vs-Rest (OvR) / One-vs-All (OvA)\n",
        "Idea:\n",
        "Train K separate binary logistic regression classifiers, one for each class.\n",
        "Each classifier distinguishes one class from all the other classes combined.\n",
        "\n",
        "How it works:\n",
        "For class\n",
        "ùëò\n",
        "k:\n",
        "\n",
        "Label samples in class\n",
        "ùëò\n",
        "k as 1 (positive).\n",
        "\n",
        "Label samples in all other classes as 0 (negative).\n",
        "Train a binary logistic regression on this setup.\n",
        "\n",
        "At prediction time:\n",
        "Run all\n",
        "ùêæ\n",
        "K classifiers on the input, each outputs a probability that the input belongs to its class.\n",
        "Pick the class with the highest probability.\n",
        "\n",
        "2. Softmax Regression (Multinomial Logistic Regression)\n",
        "Idea:\n",
        "Instead of fitting separate binary classifiers, use a single model that predicts probabilities for all classes at once.\n",
        "\n",
        "How it works:\n",
        "\n",
        "Extend the logistic function to the softmax function, which generalizes logistic regression to multiple classes.\n",
        "\n",
        "For each input\n",
        "ùë•\n",
        "x, compute a linear score for each class\n",
        "ùëò\n",
        "k:\n",
        "16. What are the advantages and disadvantages of Logistic Regression?\n",
        "- Simple and Efficient\n",
        "\n",
        "Easy to implement and computationally efficient.\n",
        "\n",
        "Works well for binary classification problems.\n",
        "\n",
        "Interpretability\n",
        "\n",
        "Coefficients are easy to interpret as odds ratios.\n",
        "\n",
        "Provides insight into the relationship between dependent and independent variables.\n",
        "\n",
        "Probabilistic Output\n",
        "\n",
        "Outputs probabilities, which are useful for ranking or decision-making thresholds.\n",
        "\n",
        "Less Prone to Overfitting\n",
        "\n",
        "With regularization (L1, L2), logistic regression can generalize well on unseen data.\n",
        "\n",
        "Works with Both Continuous and Discrete Variables\n",
        "\n",
        "Can handle a mix of feature types without much preprocessing.\n",
        "17. What are some use cases of Logistic Regression?\n",
        "- 1. Medical Diagnosis\n",
        "Predicting whether a patient has a disease (e.g., diabetes, cancer) based on diagnostic measurements or symptoms.\n",
        "\n",
        "Example: Classifying if a tumor is malignant or benign.\n",
        "\n",
        "2. Credit Scoring\n",
        "Assessing whether a loan applicant is likely to default on their loan or not.\n",
        "\n",
        "Used by banks and financial institutions to make lending decisions.\n",
        "\n",
        "3. Marketing and Customer Retention\n",
        "Predicting whether a customer will respond to a marketing campaign (yes/no).\n",
        "\n",
        "Churn prediction: Will a customer stop using a service or product?\n",
        "18. What is the difference between Softmax Regression and Logistic Regression?\n",
        "- 1. Purpose / Use Case\n",
        "Logistic Regression:\n",
        "Used for binary classification ‚Äî that is, when you have only two classes (e.g., spam vs. not spam).\n",
        "\n",
        "Softmax Regression:\n",
        "Also called multinomial logistic regression, it‚Äôs used for multi-class classification ‚Äî when you have more than two classes (e.g., classifying images into cats, dogs, or birds).\n",
        "\n",
        "2. Output\n",
        "Logistic Regression:\n",
        "Produces a single probability\n",
        "ùëù\n",
        "p for one class (usually the \"positive\" class), and the probability for the other class is\n",
        "1\n",
        "‚àí\n",
        "ùëù\n",
        "1‚àíp.\n",
        "\n",
        "Softmax Regression:\n",
        "Produces a probability distribution over multiple classes ‚Äî a vector of probabilities that sum to 1, representing the likelihood of each class.\n",
        "19.  How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification ?\n",
        "- Great question! Choosing between One-vs-Rest (OvR) and Softmax for multiclass classification depends on several factors like the problem setup, model, computational resources, and performance goals\n",
        "One-vs-Rest (OvR):\n",
        "You train K binary classifiers (one per class). Each classifier learns to distinguish one class vs. all other classes. At prediction, you run all classifiers and pick the class with the highest confidence.\n",
        "\n",
        "Softmax (Multinomial Logistic Regression):\n",
        "You train a single model that directly outputs probabilities for all classes simultaneously via the softmax function. It enforces that the class probabilities sum to 1.\n",
        "20. C How do we interpret coefficients in Logistic Regression ?\n",
        "- Coefficient (\n",
        "ùõΩ\n",
        "ùëñ\n",
        "Œ≤\n",
        "i\n",
        "‚Äã\n",
        " ) itself:\n",
        "\n",
        "If\n",
        "ùõΩ\n",
        "ùëñ\n",
        "=\n",
        "0.5\n",
        "Œ≤\n",
        "i\n",
        "‚Äã\n",
        " =0.5, a one-unit increase in\n",
        "ùëã\n",
        "ùëñ\n",
        "X\n",
        "i\n",
        "‚Äã\n",
        "  increases the log-odds by 0.5.\n",
        "\n",
        "If\n",
        "ùõΩ\n",
        "ùëñ\n",
        "=\n",
        "‚àí\n",
        "0.7\n",
        "Œ≤\n",
        "i\n",
        "‚Äã\n",
        " =‚àí0.7, a one-unit increase in\n",
        "ùëã\n",
        "ùëñ\n",
        "X\n",
        "i\n",
        "‚Äã\n",
        "  decreases the log-odds by 0.7.\n",
        "\n",
        "Exponentiated coefficient: odds ratio\n",
        "\n",
        "More interpretable is the odds ratio (OR):\n",
        "ùëí\n",
        "ùõΩ\n",
        "ùëñ\n",
        "e\n",
        "Œ≤\n",
        "i\n",
        "‚Äã\n",
        "\n",
        " .\n",
        "\n",
        "#Practical\n",
        "1. Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic\n",
        "Regression, and prints the model accuracy ?\n",
        "\n"
      ],
      "metadata": {
        "id": "Y_W4Wrk9sKXc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into train and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = model.predict(X_test)Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1')\n",
        "and print the model accuracy\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1qcn-tx1UrN",
        "outputId": "1e9af78c-d3c8-476b-8be3-17f60abddb3c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2."
      ],
      "metadata": {
        "id": "Hp192bnU1bqT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1')\n",
        "and print the model accuracy ?"
      ],
      "metadata": {
        "id": "zUerGgN31kYc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "C3q84UGt1nK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load a sample dataset (Breast cancer dataset)\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create Logistic Regression model with L1 penalty\n",
        "model = LogisticRegression(penalty='l1', solver='liblinear', random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy with L1 regularization: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pMI1o2o1vVo",
        "outputId": "d1f3b781-3f29-41b4-a9b1-fd1ab915d6cf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with L1 regularization: 0.9561\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Write a Python program to train Logistic Regression with L2 regularization (Ridge) using\n",
        "LogisticRegression(penalty='l2'). Print model accuracy and coefficients?\n"
      ],
      "metadata": {
        "id": "RSCXkcu61xUp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset (Iris dataset for example)\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# For simplicity, let's make it a binary classification problem\n",
        "# Select only two classes, e.g., class 0 and class 1\n",
        "X = X[y != 2]\n",
        "y = y[y != 2]\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Logistic Regression with L2 penalty (default)\n",
        "model = LogisticRegression(penalty='l2', solver='lbfgs')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "prin\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "EUun0y6j15s_",
        "outputId": "f0eaf9a6-0019-45e0-a9fb-66e2b4cdbb1a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'prin' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-2447935827>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mprin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'prin' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet') ?"
      ],
      "metadata": {
        "id": "zDT-9fH_1--d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create Logistic Regression model with Elastic Net regularization\n",
        "# penalty='elasticnet' requires solver='saga'\n",
        "model = LogisticRegression(penalty='elasticnet',\n",
        "                           l1_ratio=0.5,      # 0.5 means equal L1 and L2 mix\n",
        "                           solver='saga',     # saga solver supports elasticnet penalty\n"
      ],
      "metadata": {
        "id": "yFngaT4X2Fk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Write a Python program to train a Logistic Regression model for multiclass classification using\n",
        "multi_class='ovr ?\n"
      ],
      "metadata": {
        "id": "OVqi1y0j2KKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create the Logistic Regression model with multi_class='ovr'\n",
        "model = LogisticRegression(multi_class='ovr', solver='liblinear', max_iter=200)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=data.target_names))\n"
      ],
      "metadata": {
        "id": "GjVxbpHO2O_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic\n",
        "Regression. Print the best parameters and accuracy"
      ],
      "metadata": {
        "id": "W618OVZA2TX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load a sample dataset (Iris)\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the Logistic Regression model\n",
        "logreg = LogisticRegression(solver='liblinear', max_iter=1000)\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],\n",
        "    'penalty': ['l1', 'l2']\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=logreg, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit GridSearchCV\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters and score\n",
        "print(\"Best parameters:\", grid_search.best_params_)\n",
        "print(\"Best cross-validation accuracy:\", grid_search.best_score_)\n",
        "\n",
        "# Evaluate on test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test set accuracy:\", test_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFl-ZKKG2X0w",
        "outputId": "c7b4f3f0-7824-420b-83c9-478ee7d7431d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: {'C': 10, 'penalty': 'l1'}\n",
            "Best cross-validation accuracy: 0.9583333333333334\n",
            "Test set accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the\n",
        "average accuracy ?"
      ],
      "metadata": {
        "id": "YEfUuv1N5wTJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Load example dataset (Iris)\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Define Stratified K-Fold cross-validator\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "accuracies = []\n",
        "\n",
        "for train_index, test_index in skf.split(X, y):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on test data\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate accuracy and store\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(acc)\n",
        "\n",
        "# Calculate average accuracy\n",
        "average_accuracy = np.mean(accuracies)\n",
        "\n",
        "print(f'Average Accuracy: {average_accuracy:.4f}')\n"
      ],
      "metadata": {
        "id": "p1-2e-iK51PA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its\n",
        "accuracy."
      ],
      "metadata": {
        "id": "4LKm0_IN55XV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load dataset\n",
        "csv_file_path = 'your_dataset.csv'  # Replace with your CSV file path\n",
        "data = pd.read_csv(csv_file_path)\n",
        "\n",
        "# Step 2: Prepare features (X) and target (y)\n",
        "X = data.iloc[:, :-1]  # all columns except last\n",
        "y = data.iloc[:, -1]   # last column as target\n",
        "\n",
        "# Step 3: Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 4: Initialize and train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 6: Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy of Logistic Regression model: {accuracy:.2f}')\n"
      ],
      "metadata": {
        "id": "8j9Ioem06BK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in\n",
        "Logistic Regression. Print the best parameters and accuracy"
      ],
      "metadata": {
        "id": "4k8OUFxs6FSh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
        "from sklearn.metrics import acc\n"
      ],
      "metadata": {
        "id": "idx-GFlA6Lsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy"
      ],
      "metadata": {
        "id": "Vkj9M_Gz6W1v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from collections import Counter\n",
        "from itertools import combinations\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "class\n"
      ],
      "metadata": {
        "id": "JAdXcuna6ejI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary\n",
        "classificationM"
      ],
      "metadata": {
        "id": "uBaHDuGD6lUZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=10000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {acc:.2f}\")\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "labels = data.target_names\n",
        "\n",
        "# Plot confusion matrix using seaborn\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "ayq6Z4o0614a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. Write a Python program to train a Logistic Regression model and evaluate its performance using Precision,\n",
        "Recall, and F1-Score"
      ],
      "metadata": {
        "id": "7ol-N5_s7D0T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the data i\n"
      ],
      "metadata": {
        "id": "sExG1twD7fO5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}